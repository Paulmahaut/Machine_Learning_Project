# xgboost_totalenergies_full_features.py
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
import joblib
import warnings
warnings.filterwarnings('ignore')

# ---------- CONFIG ----------
INPUT_FILE = "TotalEnergies_2005_2025.xlsx"
OUTPUT_FEATURES_CSV = "tte_features.csv"
OUTPUT_MODEL = "tte_xgb_model.joblib"
PREDICTION_DAYS = 1   # horizon
TEST_RATIO = 0.2

# ---------- HELPERS ----------
def safe_float_col(df, col):
    # convert comma decimals and cast to float if exists
    if col in df.columns:
        df[col] = df[col].astype(str).str.replace(',', '.')
        df[col] = pd.to_numeric(df[col], errors='coerce')
    return df

# ---------- LOAD ----------
df = pd.read_excel(INPUT_FILE, engine='openpyxl')

# Aplatir MultiIndex si présent
if isinstance(df.columns, pd.MultiIndex):
    # on prend le niveau le plus bas (ex: 'Close' plutôt que ('Close','TTE.PA'))
    df.columns = df.columns.get_level_values(0)

# Normaliser noms colonnes (strip spaces)
df.columns = [c.strip() for c in df.columns]

# Si la colonne date s'appelle différemment, adapte ici (ici on suppose 'Date')
if 'Date' in df.columns:
    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
    df = df.set_index('Date')
else:
    # si les dates sont déjà en index ou présentes sous un autre nom,
    # tu peux adapter manuellement — ici on suppose 'Date' existe.
    pass

# Convertir l'index en DatetimeIndex (robustesse)
if not isinstance(df.index, pd.DatetimeIndex):
    df.index = pd.to_datetime(df.index, errors='coerce')

# Convertir colonnes en float (corriger virgules)
for col in ['Close', 'Open', 'High', 'Low', 'Volume']:
    df = safe_float_col(df, col)

# Drop lignes invalides
df = df.dropna(subset=['Close'])

# ---------- FEATURE ENGINEERING ----------
# Moving averages
df['MA_5']  = df['Close'].rolling(5).mean()
df['MA_20'] = df['Close'].rolling(20).mean()
df['MA_50'] = df['Close'].rolling(50).mean()

# Exponential moving averages
df['EMA_12'] = df['Close'].ewm(span=12, adjust=False).mean()
df['EMA_26'] = df['Close'].ewm(span=26, adjust=False).mean()

# MACD
df['MACD'] = df['EMA_12'] - df['EMA_26']
df['MACD_signal'] = df['MACD'].ewm(span=9, adjust=False).mean()
df['MACD_hist'] = df['MACD'] - df['MACD_signal']

# RSI (14)
delta = df['Close'].diff()
gain = delta.clip(lower=0).rolling(14).mean()
loss = (-delta.clip(upper=0)).rolling(14).mean()
rs = gain / loss
df['RSI'] = 100 - (100 / (1 + rs))

# Rate of Change
df['RoC_5'] = df['Close'].pct_change(5)
df['RoC_10'] = df['Close'].pct_change(10)

# Bollinger Bands
ma20 = df['Close'].rolling(20).mean()
std20 = df['Close'].rolling(20).std()
df['BB_upper'] = ma20 + 2 * std20
df['BB_lower'] = ma20 - 2 * std20
df['BB_width'] = df['BB_upper'] - df['BB_lower']
df['BB_position'] = (df['Close'] - df['BB_lower']) / (df['BB_upper'] - df['BB_lower'])

# ATR
df['H-L'] = df['High'] - df['Low']
df['H-PC'] = (df['High'] - df['Close'].shift(1)).abs()
df['L-PC'] = (df['Low'] - df['Close'].shift(1)).abs()
df['TR'] = df[['H-L','H-PC','L-PC']].max(axis=1)
df['ATR'] = df['TR'].rolling(14).mean()

# Rolling stats
df['Rolling_std_20'] = df['Close'].rolling(20).std()
df['Rolling_min_20'] = df['Close'].rolling(20).min()
df['Rolling_max_20'] = df['Close'].rolling(20).max()
df['Rolling_skew_20'] = df['Close'].rolling(20).skew()
df['Rolling_kurt_20'] = df['Close'].rolling(20).kurt()

# Lags
for lag in [1,2,3,5,10,20]:
    df[f'Lag_{lag}'] = df['Close'].shift(lag)

# Momentum features (court-terme)
df['Price_change_1d'] = df['Close'].diff(1)
df['Price_change_3d'] = df['Close'].diff(3)
df['Price_pct_1d'] = df['Close'].pct_change(1)
df['Price_pct_3d'] = df['Close'].pct_change(3)
df['Momentum_5'] = df['Close'] - df['Close'].shift(5)
df['Momentum_10'] = df['Close'] - df['Close'].shift(10)

# Volatility features
df['Volatility_5'] = df['Close'].rolling(5).std()
df['Volatility_20'] = df['Close'].rolling(20).std()

# Volume features (si présent)
if 'Volume' in df.columns:
    df['Volume_MA_5'] = df['Volume'].rolling(5).mean()
    df['Volume_Ratio'] = df['Volume'] / df['Volume_MA_5']

# Time features
df['Day'] = df.index.day
df['Month'] = df.index.month
df['Weekday'] = df.index.weekday
df['Is_month_end'] = df.index.is_month_end.astype(int)

# Target
df['Target'] = df['Close'].shift(-PREDICTION_DAYS)

# Drop temporaires / helper cols
helper_cols = ['H-L','H-PC','L-PC','TR']
for c in helper_cols:
    if c in df.columns:
        df.drop(columns=c, inplace=True)

# Drop lignes NA apparues après les rolling / shifts
df = df.dropna()

# Save enriched features for inspection
df.to_csv(OUTPUT_FEATURES_CSV, index=True)
print(f"Saved enriched features to {OUTPUT_FEATURES_CSV} (rows: {len(df)})")

# ---------- FEATURE LIST ----------
features = [
    'MA_5','MA_20','MA_50',
    'EMA_12','EMA_26',
    'MACD','MACD_signal','MACD_hist',
    'RSI','RoC_5','RoC_10',
    'BB_width','BB_position',
    'ATR',
    'Rolling_std_20','Rolling_min_20','Rolling_max_20',
    'Rolling_skew_20','Rolling_kurt_20',
    'Lag_1','Lag_2','Lag_3','Lag_5','Lag_10','Lag_20',
    'Price_change_1d','Price_change_3d','Price_pct_1d','Price_pct_3d',
    'Momentum_5','Momentum_10','Volatility_5','Volatility_20',
    'Day','Month','Weekday','Is_month_end'
]

if 'Volume' in df.columns:
    features += ['Volume','Volume_MA_5','Volume_Ratio']

# Ensure all features exist in df (robustness)
features = [f for f in features if f in df.columns]
features = list(dict.fromkeys(features))  # Remove duplicates while preserving order

# ---------- SPLIT ----------
split_index = int((1 - TEST_RATIO) * len(df))
X = df[features]
y = df['Target']

X_train = X.iloc[:split_index]
X_test  = X.iloc[split_index:]
y_train = y.iloc[:split_index]
y_test  = y.iloc[split_index:]

print(f"Train rows: {len(X_train)}, Test rows: {len(X_test)}")
print("Features used:", features)

# ---------- NORMALIZE FEATURES ----------
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
X_train_scaled = pd.DataFrame(X_train_scaled, columns=features, index=X_train.index)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=features, index=X_test.index)

# ---------- TRAIN XGBOOST ----------
model = XGBRegressor(
    n_estimators=800,
    max_depth=3,
    learning_rate=0.15,
    subsample=0.95,
    colsample_bytree=0.95,
    reg_alpha=0.01,  # Très peu de régularisation
    reg_lambda=0.1,
    random_state=42,
    verbosity=0
)

model.fit(X_train_scaled, y_train)

# ---------- PREDICT & METRICS ----------
preds = model.predict(X_test_scaled)
mse = mean_squared_error(y_test, preds)
rmse = np.sqrt(mse)  # Compute RMSE from MSE
mae = mean_absolute_error(y_test, preds)
r2 = r2_score(y_test, preds)

print("\nXGBoost performance:")
print(f"RMSE: {rmse:.6f}")
print(f"MAE : {mae:.6f}")
print(f"R2  : {r2:.6f}")

# ---------- SAVE MODEL ----------
joblib.dump(model, OUTPUT_MODEL)
print(f"Saved model to {OUTPUT_MODEL}")

# ---------- FEATURE IMPORTANCE ----------
importances = model.feature_importances_
fi = pd.DataFrame({'feature': features, 'importance': importances}).sort_values('importance', ascending=False)
print("\nFeature importance (top 15):")
print(fi.head(15))

# Plot feature importance
plt.figure(figsize=(10,6))
plt.barh(fi['feature'].head(20)[::-1], fi['importance'].head(20)[::-1])
plt.title("XGBoost - Feature importance")
plt.tight_layout()
plt.show()

# ---------- PLOT PREDICTIONS ----------
plt.figure(figsize=(12,6))
plt.plot(y_test.values, label='Actual', linewidth=2)
plt.plot(preds, label='XGBoost', linewidth=1.5, alpha=0.8)
plt.title(f"TotalEnergies - XGBoost Predictions ({PREDICTION_DAYS}d horizon)")
plt.xlabel("Test days")
plt.ylabel("Price")
plt.legend()
plt.grid(alpha=0.2)
plt.show()
