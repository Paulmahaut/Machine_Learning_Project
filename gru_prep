# gru_prep.py
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import joblib

# --------- LOAD enriched CSV (généré par le script XGBoost) ----------
df = pd.read_csv("tte_features.csv", index_col=0, parse_dates=True)

# On s'assure que la colonne Target existe
if 'Target' not in df.columns:
    raise ValueError("Le fichier tte_features.csv doit contenir 'Target'")

# Liste de features à utiliser (identique à XGBoost)
features = [
    'MA_5','MA_20','MA_50',
    'EMA_12','EMA_26',
    'MACD','MACD_signal','MACD_hist',
    'RSI','RoC_5','RoC_10',
    'BB_width','BB_position',
    'ATR',
    'Rolling_std_20','Rolling_min_20','Rolling_max_20',
    'Rolling_skew_20','Rolling_kurt_20',
    'Lag_1','Lag_2','Lag_3','Lag_5','Lag_10','Lag_20',
    'Day','Month','Weekday','Is_month_end'
]
if 'Volume' in df.columns:
    features += ['Volume','Volume_MA_5','Volume_Ratio']
features = [f for f in features if f in df.columns]

# Drop lignes NA si restées
df = df.dropna(subset=features + ['Target'])

# Train/test split (identique à XGBoost)
test_ratio = 0.2
split_idx = int((1 - test_ratio) * len(df))
df_train = df.iloc[:split_idx]
df_test  = df.iloc[split_idx:]

# Scaling (important pour réseaux)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(df_train[features])
X_test_scaled  = scaler.transform(df_test[features])

# Save scaler for later
joblib.dump(scaler, "scaler_tte.joblib")

# We will build sequences for GRU
sequence_length = 30  # window size

def build_sequences(X_scaled, y, seq_len):
    Xs, ys = [], []
    for i in range(len(X_scaled) - seq_len + 1):
        Xs.append(X_scaled[i:i+seq_len])
        ys.append(y[i+seq_len-1])  # predict based on last element of window
    return np.array(Xs), np.array(ys)

# For training: we must use continuous slices; ensure matching indices
y_train = df_train['Target'].values
y_test  = df_test['Target'].values

# Because we built sequences from start of dataset, adjust arrays indexes accordingly
X_train_seq, y_train_seq = build_sequences(X_train_scaled, y_train, sequence_length)
# For test sequences: we need to use the tail of the training frames + test frames to form sequences
# Simpler: build sequences on the concatenation of last (seq_len-1) train rows + test rows
tail = X_train_scaled[-(sequence_length-1):] if sequence_length-1 > 0 else np.empty((0, X_train_scaled.shape[1]))
X_test_concat = np.vstack([tail, X_test_scaled])
y_test_concat = np.concatenate([df_train['Target'].values[-(sequence_length-1):], y_test]) if sequence_length-1 > 0 else y_test
X_test_seq, y_test_seq = build_sequences(X_test_concat, y_test_concat, sequence_length)

print("GRU shapes:", X_train_seq.shape, y_train_seq.shape, X_test_seq.shape, y_test_seq.shape)

# Save prepared arrays
np.save("X_train_seq.npy", X_train_seq)
np.save("y_train_seq.npy", y_train_seq)
np.save("X_test_seq.npy", X_test_seq)
np.save("y_test_seq.npy", y_test_seq)
