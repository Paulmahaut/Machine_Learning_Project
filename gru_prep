"""
GRU Data Preparation for Tesla Stock Prediction
================================================
Ce script pr√©pare les donn√©es pour un r√©seau GRU (Gated Recurrent Unit).

Diff√©rence cl√© avec XGBoost:
- XGBoost: chaque ligne = 1 pr√©diction ind√©pendante
- GRU: chaque pr√©diction utilise une S√âQUENCE de N jours pr√©c√©dents

Exemple: pour pr√©dire le prix du jour 100, GRU regarde les jours 70-99 (s√©quence de 30)
"""

import yfinance as yf
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import joblib


# ========== STEP 1: T√©l√©charger et cr√©er les features (comme XGBoost v2) ==========
print("üì• T√©l√©chargement donn√©es Tesla...")
df = yf.download("TSLA", start="2015-01-01", end="2025-01-01", progress=False)

if isinstance(df.columns, pd.MultiIndex):
    df.columns = df.columns.get_level_values(0)

print(f"‚úÖ {len(df)} jours t√©l√©charg√©s")


# ========== STEP 2: Cr√©er les features (identiques √† xgboost_simple2.py) ==========
print("üîß Cr√©ation des 24 features...")

# Base features
df['MA_5'] = df['Close'].rolling(5).mean()
df['MA_20'] = df['Close'].rolling(20).mean()
df['Lag_1'] = df['Close'].shift(1)
df['Lag_2'] = df['Close'].shift(2)
df['Lag_3'] = df['Close'].shift(3)
df['Volatility'] = df['Close'].pct_change().rolling(20).std()

# Bollinger Bands
ma20 = df['Close'].rolling(20).mean()
std20 = df['Close'].rolling(20).std()
df['BB_upper'] = ma20 + 2 * std20
df['BB_lower'] = ma20 - 2 * std20
df['BB_width'] = df['BB_upper'] - df['BB_lower']
df['BB_position'] = (df['Close'] - df['BB_lower']) / (df['BB_upper'] - df['BB_lower'])

# Price Changes
df['Price_change_1d'] = df['Close'].diff(1)
df['Price_change_3d'] = df['Close'].diff(3)
df['Price_pct_1d'] = df['Close'].pct_change(1)
df['Price_pct_3d'] = df['Close'].pct_change(3)

# Rate of Change
df['RoC_5'] = df['Close'].pct_change(5)
df['RoC_10'] = df['Close'].pct_change(10)

# Momentum
df['Momentum_5'] = df['Close'] - df['Close'].shift(5)
df['Momentum_10'] = df['Close'] - df['Close'].shift(10)

# MACD
df['EMA_12'] = df['Close'].ewm(span=12, adjust=False).mean()
df['EMA_26'] = df['Close'].ewm(span=26, adjust=False).mean()
df['MACD'] = df['EMA_12'] - df['EMA_26']
df['MACD_signal'] = df['MACD'].ewm(span=9, adjust=False).mean()
df['MACD_hist'] = df['MACD'] - df['MACD_signal']

# ATR
df['H-L'] = df['High'] - df['Low']
df['H-PC'] = (df['High'] - df['Close'].shift(1)).abs()
df['L-PC'] = (df['Low'] - df['Close'].shift(1)).abs()
df['TR'] = df[['H-L','H-PC','L-PC']].max(axis=1)
df['ATR'] = df['TR'].rolling(14).mean()

# Volume
df['Volume_MA_5'] = df['Volume'].rolling(5).mean()
df['Volume_Ratio'] = df['Volume'] / df['Volume_MA_5']

# Target (5 jours √† l'avance)
df['Target'] = df['Close'].shift(-5)

# Liste des 24 features
features = [
    'MA_5', 'MA_20', 'Lag_1', 'Lag_2', 'Lag_3', 'Volatility',
    'BB_width', 'BB_position',
    'Price_change_1d', 'Price_change_3d', 'Price_pct_1d', 'Price_pct_3d',
    'RoC_5', 'RoC_10',
    'Momentum_5', 'Momentum_10',
    'MACD', 'MACD_signal', 'MACD_hist',
    'ATR',
    'Volume_MA_5', 'Volume_Ratio',
    'EMA_12', 'EMA_26'
]

# Supprimer les NaN
df = df.dropna()
print(f"‚úÖ Features cr√©√©es, {len(df)} jours valides apr√®s nettoyage")


# ========== STEP 3: Split Train/Test (chronologique 80/20) ==========
split_idx = int(len(df) * 0.8)
df_train = df.iloc[:split_idx]
df_test = df.iloc[split_idx:]

print(f"üìä Train: {len(df_train)} jours | Test: {len(df_test)} jours")


# ========== STEP 4: Normalisation (CRUCIAL pour r√©seaux de neurones) ==========
"""
Pourquoi normaliser?
- Les features ont des √©chelles diff√©rentes (prix en $, volume en millions, % en 0-1)
- Les r√©seaux de neurones apprennent mieux avec des valeurs entre -1 et 1
- StandardScaler: transforme chaque feature pour avoir moyenne=0 et √©cart-type=1

Formule: x_scaled = (x - moyenne) / √©cart-type
"""
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(df_train[features])  # Fit sur train uniquement!
X_test_scaled = scaler.transform(df_test[features])        # Applique la m√™me transformation

# Sauvegarder le scaler (pour faire des pr√©dictions futures)
joblib.dump(scaler, "scaler_tsla.joblib")
print("‚úÖ Donn√©es normalis√©es et scaler sauvegard√©")


# ========== STEP 5: Cr√©er les s√©quences pour GRU ==========
"""
GRU = r√©seau r√©current qui "se souvient" des jours pr√©c√©dents

Concept de s√©quence:
- Au lieu de donner 1 jour ‚Üí pr√©dire prix futur
- On donne 30 jours ‚Üí pr√©dire prix futur
- Le GRU lit ces 30 jours un par un et "m√©morise" les patterns

Exemple concret:
  S√©quence 1: jours 1-30   ‚Üí pr√©dit jour 30+5 = jour 35
  S√©quence 2: jours 2-31   ‚Üí pr√©dit jour 31+5 = jour 36
  S√©quence 3: jours 3-32   ‚Üí pr√©dit jour 32+5 = jour 37
  ...

IMPORTANT: On utilise une fen√™tre glissante (sliding window)
"""
SEQUENCE_LENGTH = 30  # Nombre de jours dans chaque s√©quence


def build_sequences(X_scaled, y_values, seq_len):
    """
    Construit des s√©quences temporelles √† partir des features normalis√©es
    
    Parameters:
    -----------
    X_scaled : array (n_days, n_features)
        Features normalis√©es
    y_values : array (n_days,)
        Valeurs cibles
    seq_len : int
        Longueur de chaque s√©quence
    
    Returns:
    --------
    X_sequences : array (n_sequences, seq_len, n_features)
        S√©quences 3D pour le GRU
    y_sequences : array (n_sequences,)
        Cibles correspondantes
    """
    X_sequences = []
    y_sequences = []
    
    # Pour chaque position possible dans les donn√©es
    for i in range(len(X_scaled) - seq_len + 1):
        # Extraire une fen√™tre de seq_len jours
        sequence = X_scaled[i : i + seq_len]
        # La cible = prix futur du dernier jour de la s√©quence
        target = y_values[i + seq_len - 1]
        
        X_sequences.append(sequence)
        y_sequences.append(target)
    
    return np.array(X_sequences), np.array(y_sequences)


# Cr√©er les s√©quences pour le train
y_train = df_train['Target'].values
X_train_seq, y_train_seq = build_sequences(X_train_scaled, y_train, SEQUENCE_LENGTH)

print(f"üîÑ Train sequences: {X_train_seq.shape}")
print(f"   ‚îî‚îÄ {X_train_seq.shape[0]} s√©quences de {X_train_seq.shape[1]} jours avec {X_train_seq.shape[2]} features")


# ========== STEP 6: S√©quences de test (ATTENTION: continuit√© temporelle) ==========
"""
PI√àGE √† √©viter:
- Si on cr√©e les s√©quences test ind√©pendamment, la premi√®re s√©quence test 
  n'aura pas acc√®s aux derniers jours du train
- Or en r√©alit√©, pour pr√©dire le jour 2001, on a besoin des jours 1971-2000

Solution:
- Concat√©ner les 29 derniers jours du train avec le test
- Ainsi la premi√®re s√©quence test utilise: [train[-29:], test[0]] ‚Üí 30 jours
"""
# Prendre les (SEQUENCE_LENGTH-1) derniers jours du train
tail_train = X_train_scaled[-(SEQUENCE_LENGTH - 1):]
y_tail_train = df_train['Target'].values[-(SEQUENCE_LENGTH - 1):]

# Concat√©ner avec le test
X_test_concat = np.vstack([tail_train, X_test_scaled])
y_test_concat = np.concatenate([y_tail_train, df_test['Target'].values])

# Cr√©er les s√©quences
X_test_seq, y_test_seq = build_sequences(X_test_concat, y_test_concat, SEQUENCE_LENGTH)

print(f"üîÑ Test sequences: {X_test_seq.shape}")
print(f"   ‚îî‚îÄ {X_test_seq.shape[0]} s√©quences de {X_test_seq.shape[1]} jours avec {X_test_seq.shape[2]} features")


# ========== STEP 7: Sauvegarder les donn√©es pr√©par√©es ==========
np.save("X_train_seq.npy", X_train_seq)
np.save("y_train_seq.npy", y_train_seq)
np.save("X_test_seq.npy", X_test_seq)
np.save("y_test_seq.npy", y_test_seq)

print("\n‚úÖ Pr√©paration termin√©e!")
print(f"üìÅ Fichiers sauvegard√©s:")
print(f"   - X_train_seq.npy: {X_train_seq.shape}")
print(f"   - y_train_seq.npy: {y_train_seq.shape}")
print(f"   - X_test_seq.npy: {X_test_seq.shape}")
print(f"   - y_test_seq.npy: {y_test_seq.shape}")
print(f"   - scaler_tsla.joblib")
print(f"\n‚û°Ô∏è Prochaine √©tape: ex√©cuter gru_train.py")
