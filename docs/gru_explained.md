# GRU Model ‚Äî Guide p√©dagogique complet

## üìñ Introduction

### Qu'est-ce qu'un GRU?

**GRU = Gated Recurrent Unit** (Unit√© R√©currente √† Portes)

C'est un type de r√©seau de neurones sp√©cialis√© pour les **donn√©es s√©quentielles** comme:
- S√©ries temporelles (prix d'actions, m√©t√©o, etc.)
- Texte (traduction, g√©n√©ration)
- Audio (reconnaissance vocale)

### Diff√©rence cl√© vs XGBoost

| Aspect | XGBoost | GRU |
|--------|---------|-----|
| Type | Arbres de d√©cision | R√©seau de neurones r√©current |
| Input | 1 jour = 1 pr√©diction | S√©quence de N jours ‚Üí 1 pr√©diction |
| M√©moire | Aucune (chaque jour ind√©pendant) | "Se souvient" des jours pr√©c√©dents |
| Complexit√© | Moyenne | √âlev√©e |

### Exemple concret

**XGBoost:**
```
Jour 100 (features du jour 100) ‚Üí Pr√©dit jour 105
Jour 101 (features du jour 101) ‚Üí Pr√©dit jour 106
```

**GRU:**
```
Jours 70-99 (s√©quence de 30) ‚Üí Pr√©dit jour 105
Jours 71-100 (s√©quence de 30) ‚Üí Pr√©dit jour 106
```

Le GRU "lit" 30 jours d'affil√©e et capture les **patterns temporels** (tendances, cycles).

---

## üîß Pr√©paration des donn√©es (gru_prep.py)

### √âtape 1: Normalisation (CRUCIAL!)

```python
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(df_train[features])
```

**Pourquoi normaliser?**
- Les features ont des √©chelles tr√®s diff√©rentes:
  - Prix: $200-$400
  - Volume: 10M-200M
  - Pourcentages: -0.1 √† +0.1
- Les r√©seaux de neurones apprennent mieux avec des valeurs similaires
- StandardScaler transforme tout pour avoir **moyenne=0** et **√©cart-type=1**

**Formule:**
```
x_normalized = (x - moyenne) / √©cart-type
```

**Exemple:**
```
Prix brut: $250
Moyenne des prix: $300
√âcart-type: $50
Prix normalis√©: ($250 - $300) / $50 = -1.0
```

### √âtape 2: Cr√©ation des s√©quences

**Concept de fen√™tre glissante (sliding window):**

Donn√©es brutes (apr√®s normalisation):
```
Jour 1: [feature1, feature2, ..., feature24]
Jour 2: [feature1, feature2, ..., feature24]
...
Jour 30: [feature1, feature2, ..., feature24]
Jour 31: [feature1, feature2, ..., feature24]
```

S√©quences cr√©√©es (SEQUENCE_LENGTH = 30):
```
S√©quence 1: Jours 1-30   (30 jours √ó 24 features) ‚Üí Target du jour 30
S√©quence 2: Jours 2-31   (30 jours √ó 24 features) ‚Üí Target du jour 31
S√©quence 3: Jours 3-32   (30 jours √ó 24 features) ‚Üí Target du jour 32
...
```

**Shape des tenseurs:**
- Input: `(n_sequences, 30, 24)`
  - `n_sequences`: nombre de s√©quences
  - `30`: longueur de chaque s√©quence (jours)
  - `24`: nombre de features par jour
- Output: `(n_sequences,)` (un prix futur par s√©quence)

### √âtape 3: Continuit√© temporelle pour le test

**PI√àGE √† √©viter:**
Si on cr√©e les s√©quences test ind√©pendamment, la premi√®re s√©quence test manquerait de contexte.

**Solution:**
Concat√©ner les 29 derniers jours du train avec le test:
```
Train: [..., jour 1990, jour 1991, jour 1992]
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚Üì Copie
Test concat: [jour 1964-1992, jour 1993, jour 1994, ...]
             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ 29 jours ‚îÄ‚îÄ‚îò
```

Ainsi, la premi√®re s√©quence test = jours 1964-1993 (compl√®te!).

---

## üèóÔ∏è Architecture du mod√®le (gru_train.py)

### Couche 1: GRU(64, return_sequences=True)

```python
GRU(64, return_sequences=True, input_shape=(30, 24))
```

**Que fait cette couche?**
1. Lit les 30 jours de la s√©quence **un par un**
2. Pour chaque jour, met √† jour un "√©tat cach√©" (m√©moire) de taille 64
3. `return_sequences=True` ‚Üí retourne l'√©tat √† chaque pas de temps

**Analogie:**
Imagine un lecteur qui lit un livre de 30 pages:
- Page 1 ‚Üí Se souvient des infos importantes (64 notes mentales)
- Page 2 ‚Üí Met √† jour ses notes (64 valeurs)
- ...
- Page 30 ‚Üí A 30 ensembles de notes (un par page)

**Output shape:** `(batch, 30, 64)`
- `30` = un √©tat pour chaque jour
- `64` = taille de la m√©moire

### Dropout(0.2)

```python
Dropout(0.2)
```

**R√©gularisation:**
- D√©sactive al√©atoirement 20% des neurones pendant l'entra√Ænement
- Force le r√©seau √† ne pas d√©pendre de neurones sp√©cifiques
- R√©duit l'overfitting (comme `subsample` dans XGBoost)

### Couche 2: GRU(32, return_sequences=False)

```python
GRU(32, return_sequences=False)
```

**Que fait cette couche?**
1. Prend les 30 √©tats de la couche pr√©c√©dente
2. Les condense en UN seul √©tat final de taille 32
3. `return_sequences=False` ‚Üí retourne seulement le dernier √©tat

**Analogie:**
Le lecteur relit ses 30 pages de notes et √©crit un **r√©sum√© final** (32 points cl√©s).

**Output shape:** `(batch, 32)`

### Couche 3: Dense(16, relu)

```python
Dense(16, activation='relu')
```

**Couche fully-connected:**
- Combine les 32 valeurs du GRU
- Cr√©e 16 nouvelles features
- `relu = max(0, x)` ‚Üí activation non-lin√©aire

### Couche 4: Dense(1, linear)

```python
Dense(1, activation='linear')
```

**Sortie finale:**
- 1 neurone = le prix pr√©dit
- `linear` = pas d'activation (r√©gression)

### Architecture compl√®te

```
Input: (30 jours, 24 features)
   ‚Üì
GRU(64) ‚Üí Dropout ‚Üí √âtats: (30, 64)
   ‚Üì
GRU(32) ‚Üí Dropout ‚Üí √âtat final: (32,)
   ‚Üì
Dense(16, relu) ‚Üí Features: (16,)
   ‚Üì
Dense(1, linear) ‚Üí Pr√©diction: 1 prix
```

**Param√®tres totaux:** 27,233
- Beaucoup moins que des mod√®les modernes (millions)
- Assez pour capturer les patterns de Tesla

---

## üìä Entra√Ænement

### Loss Function: MSE

```python
loss='mse'  # Mean Squared Error
```

**Formule:**
```
MSE = Moyenne((Pr√©diction - R√©el)¬≤)
```

**Identique √† XGBoost!**

### Optimizer: Adam

```python
optimizer=Adam(learning_rate=0.001)
```

**Adam = Adaptive Moment Estimation**
- Ajuste automatiquement le taux d'apprentissage pour chaque param√®tre
- Plus sophistiqu√© que SGD (Stochastic Gradient Descent)
- `learning_rate=0.001` = valeur standard

### Callbacks

#### EarlyStopping
```python
EarlyStopping(monitor='val_loss', patience=10)
```

**Fonctionnement:**
1. √Ä chaque epoch, calcule `val_loss` (erreur sur validation)
2. Si `val_loss` ne s'am√©liore pas pendant 10 epochs ‚Üí STOP
3. Restaure les poids du meilleur epoch

**Analogie avec XGBoost:**
Comme XGBoost arr√™te d'ajouter des arbres si √ßa n'am√©liore plus.

#### ModelCheckpoint
```python
ModelCheckpoint('gru_tsla_best.h5', monitor='val_loss', save_best_only=True)
```

**Sauvegarde automatique:**
- √Ä chaque am√©lioration de `val_loss` ‚Üí sauvegarde le mod√®le
- Garde uniquement le meilleur

---

## üìà R√©sultats obtenus

### M√©triques

```
GRU  ‚Üí R¬≤=0.5926 | RMSE=$37.10 | MAE=$26.34 | Gap=0.3854
XGB  ‚Üí R¬≤=0.8309 | RMSE=$23.90 | MAE=$18.15 | Gap=0.1659
```

### Analyse

#### R¬≤ = 0.5926
- Le mod√®le explique ~59% de la variance
- **Moins bon que XGBoost (83%)**
- Pas terrible mais acceptable pour un premier essai

#### RMSE = $37.10
- Erreur moyenne de ~$37
- **Moins bon que XGBoost ($24)**
- Sur Tesla √† $250, √ßa fait ~15% d'erreur

#### Gap = 0.3854
- **PROBL√àME: Overfitting √©lev√©**
- R¬≤ train = 0.9780 (98% sur train!)
- R¬≤ test = 0.5926 (59% sur test)
- Le mod√®le "m√©morise" le train mais g√©n√©ralise mal

### Pourquoi le GRU est moins bon?

1. **Overfitting:**
   - R√©seau trop complexe pour la quantit√© de donn√©es
   - 27,233 param√®tres vs 1,963 s√©quences train

2. **March√©s financiers:**
   - Tr√®s bruit√©s et chaotiques
   - Les RNNs ont du mal avec les changements brusques
   - XGBoost g√®re mieux les non-lin√©arit√©s locales

3. **S√©quences courtes:**
   - 30 jours peut-√™tre insuffisant pour capturer les cycles longs
   - Ou trop long (introduit du bruit)

---

## üîß Pistes d'am√©lioration

### 1. R√©duire l'overfitting

**Augmenter le Dropout:**
```python
Dropout(0.3)  # au lieu de 0.2
Dropout(0.4)  # encore plus agressif
```

**R√©duire la complexit√©:**
```python
GRU(32, ...)  # au lieu de 64
GRU(16, ...)  # au lieu de 32
```

**Ajouter de la r√©gularisation L2:**
```python
from tensorflow.keras.regularizers import l2
GRU(64, kernel_regularizer=l2(0.01), ...)
```

### 2. Ajuster la longueur de s√©quence

**Tester diff√©rentes longueurs:**
```python
SEQUENCE_LENGTH = 20  # Plus court
SEQUENCE_LENGTH = 60  # Plus long
```

**Trade-off:**
- Court (10-20): Moins d'overfitting, perd patterns longs
- Long (60-90): Capture cycles longs, risque overfitting

### 3. Utiliser LSTM au lieu de GRU

**LSTM = Long Short-Term Memory**
- Plus complexe que GRU
- Meilleur pour d√©pendances tr√®s longues
- Plus lent √† entra√Æner

```python
from tensorflow.keras.layers import LSTM
model.add(LSTM(64, return_sequences=True, ...))
```

### 4. Augmenter les donn√©es

**Plus de donn√©es = moins d'overfitting**
- T√©l√©charger depuis 2010 (15 ans au lieu de 10)
- Ajouter d'autres actions similaires (NVDA, AMD)

### 5. Ensembling

**Combiner GRU + XGBoost:**
```python
prediction_finale = 0.3 * prediction_gru + 0.7 * prediction_xgb
```

---

## üìö Concepts cl√©s pour l'√©valuation

### Pourquoi GRU pour s√©ries temporelles?

**M√©moire temporelle:**
- Les prix d'actions ont une d√©pendance temporelle
- Le prix d'aujourd'hui d√©pend de hier, avant-hier, etc.
- GRU "se souvient" de ces d√©pendances via son √©tat cach√©

**Exemple:**
```
Tendance haussi√®re sur 20 jours ‚Üí GRU apprend "momentum positif"
Forte chute r√©cente ‚Üí GRU ajuste sa pr√©diction √† la baisse
```

### GRU vs LSTM

| Aspect | GRU | LSTM |
|--------|-----|------|
| Complexit√© | Simpler (2 portes) | Plus complexe (3 portes) |
| Param√®tres | Moins | Plus |
| Vitesse | Plus rapide | Plus lent |
| Performance | Souvent √©quivalente | Meilleur sur s√©quences tr√®s longues |

**Pour Tesla (30 jours), GRU suffit.**

### Normalisation: pourquoi essentielle?

**Sans normalisation:**
```
Prix: 250
Volume: 100,000,000
Volatility: 0.02

Gradient du r√©seau ‚Üí domin√© par Volume (√©norme)
‚Üí Apprentissage inefficace
```

**Avec normalisation:**
```
Prix normalis√©: -0.5
Volume normalis√©: 1.2
Volatility normalis√©: -0.8

Gradient √©quilibr√© ‚Üí Apprentissage efficace
```

### EarlyStopping: pourquoi important?

**Sans EarlyStopping:**
```
Epoch 1-30: val_loss diminue
Epoch 31-50: val_loss stagne
Epoch 51-100: val_loss augmente (overfitting!)
‚Üí Perte de temps + overfitting
```

**Avec EarlyStopping (patience=10):**
```
Epoch 1-30: val_loss diminue
Epoch 31-40: val_loss stagne (patience compteur: 1-10)
Epoch 40: STOP ‚Üí Restaure epoch 30
‚Üí √âconomise temps + √©vite overfitting
```

---

## üéØ Conclusion

### Forces du GRU

‚úÖ Capture d√©pendances temporelles (tendances, momentum)
‚úÖ Architecture claire et interpr√©table
‚úÖ Fonctionne (R¬≤=0.59 pas catastrophique)

### Faiblesses observ√©es

‚ùå Overfitting √©lev√© (gap=0.38)
‚ùå Moins performant que XGBoost
‚ùå N√©cessite beaucoup de tuning

### Quand utiliser GRU vs XGBoost?

**Utiliser XGBoost si:**
- Donn√©es tabulaires avec features engineered
- Besoin de performance maximale
- Interpr√©tabilit√© importante (feature importance)

**Utiliser GRU si:**
- S√©quences naturelles (texte, audio, vid√©o)
- Patterns temporels complexes
- Pas le temps de faire du feature engineering

**Pour Tesla:**
XGBoost gagne car features bien choisies (MA, EMA, MACD, etc.) + moins d'overfitting.

---

## üìñ Ressources

- [Understanding GRU Networks](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be)
- [Keras GRU Documentation](https://keras.io/api/layers/recurrent_layers/gru/)
- [Time Series Forecasting with Deep Learning](https://machinelearningmastery.com/time-series-forecasting-with-deep-learning/)
