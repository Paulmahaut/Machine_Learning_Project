"""
GRU Model Training for Tesla Stock Prediction
==============================================
Ce script entra√Æne un r√©seau GRU (Gated Recurrent Unit) sur les s√©quences pr√©par√©es.

Qu'est-ce qu'un GRU?
- Type de r√©seau de neurones r√©current (RNN)
- Con√ßu pour les donn√©es s√©quentielles (texte, s√©ries temporelles)
- "M√©morise" les informations importantes des jours pr√©c√©dents
- Plus simple que LSTM mais souvent aussi performant

Comment il fonctionne?
1. Lit une s√©quence de 30 jours, jour par jour
2. √Ä chaque jour, met √† jour sa "m√©moire" (√©tat cach√©)
3. Apr√®s 30 jours, utilise cette m√©moire pour pr√©dire le prix futur
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')


# ========== STEP 1: Charger les s√©quences pr√©par√©es ==========
print("üìÇ Chargement des donn√©es...")
X_train = np.load("X_train_seq.npy")
y_train = np.load("y_train_seq.npy")
X_test = np.load("X_test_seq.npy")
y_test = np.load("y_test_seq.npy")

print(f"‚úÖ Donn√©es charg√©es:")
print(f"   Train: {X_train.shape} ‚Üí {y_train.shape}")
print(f"   Test:  {X_test.shape} ‚Üí {y_test.shape}")


# ========== STEP 2: D√©finir les param√®tres ==========
n_timesteps = X_train.shape[1]  # 30 jours
n_features = X_train.shape[2]   # 24 features

# Hyperparam√®tres
BATCH_SIZE = 32   # Nombre d'exemples trait√©s ensemble
EPOCHS = 100      # Nombre de passages sur toutes les donn√©es
LEARNING_RATE = 0.001  # Taux d'apprentissage (0.001 = standard)

print(f"\nüìä Configuration:")
print(f"   S√©quence: {n_timesteps} jours")
print(f"   Features: {n_features}")
print(f"   Batch size: {BATCH_SIZE}")
print(f"   Epochs max: {EPOCHS}")


# ========== STEP 3: Construire l'architecture GRU ==========
"""
Architecture choisie: 2 couches GRU + 2 couches Dense

Couche 1: GRU(64)
  - 64 unit√©s = taille de la m√©moire
  - return_sequences=True ‚Üí passe la s√©quence compl√®te √† la couche suivante
  - Input: (batch, 30 jours, 24 features)
  - Output: (batch, 30 jours, 64 unit√©s)

Dropout(0.2)
  - D√©sactive al√©atoirement 20% des neurones
  - Emp√™che l'overfitting (comme subsample dans XGBoost)

Couche 2: GRU(32)
  - 32 unit√©s (plus petit pour r√©duire complexit√©)
  - return_sequences=False ‚Üí retourne seulement le dernier √©tat
  - Output: (batch, 32 unit√©s)

Dropout(0.2)

Dense(16, relu)
  - Couche fully-connected de 16 neurones
  - relu = max(0, x) ‚Üí activation non-lin√©aire

Dense(1, linear)
  - Sortie finale: 1 neurone = prix pr√©dit
  - linear = pas d'activation (regression)
"""
print("\nüèóÔ∏è Construction du mod√®le...")

model = Sequential([
    # Premi√®re couche GRU (analyse la s√©quence)
    GRU(64, return_sequences=True, input_shape=(n_timesteps, n_features)),
    Dropout(0.2),
    
    # Deuxi√®me couche GRU (extrait les patterns finaux)
    GRU(32, return_sequences=False),
    Dropout(0.2),
    
    # Couches denses pour la pr√©diction finale
    Dense(16, activation='relu'),
    Dense(1, activation='linear')  # Pr√©diction du prix
])

# Compilation: d√©finit comment le mod√®le apprend
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
    loss='mse',      # Mean Squared Error (comme XGBoost)
    metrics=['mae']  # Mean Absolute Error (suivi pendant l'entra√Ænement)
)

# Afficher l'architecture
model.summary()
print("\n‚úÖ Mod√®le cr√©√©")


# ========== STEP 4: D√©finir les callbacks ==========
"""
Callbacks = actions automatiques pendant l'entra√Ænement

EarlyStopping:
  - Surveille val_loss (erreur sur validation)
  - Si val_loss ne s'am√©liore pas pendant 10 epochs ‚Üí arr√™te
  - restore_best_weights=True ‚Üí garde les meilleurs poids
  - √âvite l'overfitting (comme XGBoost qui arr√™te si arbres n'am√©liorent plus)

ModelCheckpoint:
  - Sauvegarde le mod√®le √† chaque fois que val_loss s'am√©liore
  - Garde uniquement le meilleur mod√®le
"""
callbacks = [
    EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True,
        verbose=1
    ),
    ModelCheckpoint(
        'gru_tsla_best.h5',
        monitor='val_loss',
        save_best_only=True,
        verbose=0
    )
]

print("\nüìå Callbacks configur√©s:")
print("   - Early Stopping: patience=10 epochs")
print("   - Model Checkpoint: sauvegarde du meilleur mod√®le")


# ========== STEP 5: Entra√Æner le mod√®le ==========
"""
Entra√Ænement:
- validation_split=0.1 ‚Üí 10% du train devient validation
- Le mod√®le voit train (90%), s'√©value sur validation (10%), jamais sur test
- verbose=2 ‚Üí affiche 1 ligne par epoch
"""
print("\nüöÄ D√©but de l'entra√Ænement...")
print("=" * 60)

history = model.fit(
    X_train, y_train,
    validation_split=0.1,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    callbacks=callbacks,
    verbose=2
)

print("=" * 60)
print("‚úÖ Entra√Ænement termin√©\n")


# ========== STEP 6: √âvaluer sur le test set ==========
print("üìà √âvaluation sur le test set...")

# Pr√©dictions
predictions = model.predict(X_test, verbose=0).flatten()

# M√©triques (identiques √† XGBoost)
rmse = np.sqrt(mean_squared_error(y_test, predictions))
mae = mean_absolute_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

print(f"\nüéØ R√©sultats GRU:")
print(f"   R¬≤ = {r2:.4f}")
print(f"   RMSE = ${rmse:.2f}")
print(f"   MAE = ${mae:.2f}")


# ========== STEP 7: Calculer l'overfitting ==========
"""
Comme pour XGBoost, on compare train vs test
"""
# Pr√©dire sur train pour voir l'overfitting
train_preds = model.predict(X_train, verbose=0).flatten()
r2_train = r2_score(y_train, train_preds)
gap = r2_train - r2

print(f"\nüìä Overfitting:")
print(f"   R¬≤ train = {r2_train:.4f}")
print(f"   R¬≤ test  = {r2:.4f}")
print(f"   Gap      = {gap:.4f}")

if gap < 0.15:
    print("   ‚úÖ Overfitting faible (bon!)")
elif gap < 0.25:
    print("   ‚ö†Ô∏è Overfitting mod√©r√© (acceptable)")
else:
    print("   ‚ùå Overfitting √©lev√© (r√©duire complexit√©)")


# ========== STEP 8: Sauvegarder le mod√®le final ==========
model.save("gru_tsla_final.h5")
print(f"\nüíæ Mod√®le sauvegard√©: gru_tsla_final.h5")


# ========== STEP 9: Visualiser l'entra√Ænement ==========
plt.figure(figsize=(12, 5))

# Subplot 1: Loss pendant l'entra√Ænement
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss', linewidth=2)
plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)
plt.xlabel('Epoch')
plt.ylabel('MSE Loss')
plt.title('Training History')
plt.legend()
plt.grid(alpha=0.3)

# Subplot 2: Pr√©dictions vs R√©alit√©
plt.subplot(1, 2, 2)
N = min(200, len(y_test))  # Afficher 200 premiers points
plt.plot(y_test[:N], label='Prix r√©el', alpha=0.7, linewidth=2, color='blue')
plt.plot(predictions[:N], label='Pr√©dictions GRU', alpha=0.7, linestyle='--', linewidth=2, color='orange')
plt.xlabel('Jours (test set)')
plt.ylabel('Prix ($)')
plt.title(f'Tesla - GRU Predictions (R¬≤={r2:.4f})')
plt.legend()
plt.grid(alpha=0.3)

plt.tight_layout()
plt.savefig('TSLA_gru_results.png', dpi=150, bbox_inches='tight')
plt.show()

print("üìä Graphiques sauvegard√©s: TSLA_gru_results.png")


# ========== STEP 10: Sauvegarder les pr√©dictions ==========
results_df = pd.DataFrame({
    'y_true': y_test,
    'y_pred_gru': predictions,
    'error': y_test - predictions,
    'error_pct': ((y_test - predictions) / y_test * 100)
})
results_df.to_csv("gru_predictions.csv", index=False)

print("üìÅ Pr√©dictions sauvegard√©es: gru_predictions.csv")


# ========== STEP 11: Comparaison avec XGBoost (si disponible) ==========
print("\n" + "=" * 60)
print("üìä R√âCAPITULATIF")
print("=" * 60)
print(f"GRU  ‚Üí R¬≤={r2:.4f} | RMSE=${rmse:.2f} | MAE=${mae:.2f} | Gap={gap:.4f}")
print(f"XGB  ‚Üí R¬≤=0.8309 | RMSE=$23.90 | MAE=$18.15 | Gap=0.1659")
print("=" * 60)
print("\n‚úÖ Entra√Ænement GRU termin√©!")
